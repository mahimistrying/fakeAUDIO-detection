{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c3bdc7-07db-4ec6-9d0b-30a1849d0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define new directories for your dataset\n",
    "new_real_dir = r'C:/mahimisTrying/FInalYearProject/newNew/Audio-DeepFake-Detection/Audio-DeepFake-Detection/data/New folder/Untitled Folder/gitpull/mp3towav/datasetforKerasmodel/real'\n",
    "new_fake_dir = r'C:/mahimisTrying/FInalYearProject/newNew/Audio-DeepFake-Detection/Audio-DeepFake-Detection/data/New folder/Untitled Folder/gitpull/mp3towav/datasetforKerasmodel/fake'\n",
    "\n",
    "# Load audio files function\n",
    "def load_audio_files(directory, label):\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if file_path.endswith(\".wav\"):\n",
    "            audio, sr = librosa.load(file_path, sr=16000)\n",
    "            audio_files.append(audio)\n",
    "            labels.append(label)\n",
    "    return audio_files, labels\n",
    "\n",
    "# Load new real and fake audio data\n",
    "new_real_audio, new_real_labels = load_audio_files(new_real_dir, 0)\n",
    "new_fake_audio, new_fake_labels = load_audio_files(new_fake_dir, 1)\n",
    "\n",
    "# Combine new real and fake data\n",
    "X_new = new_real_audio + new_fake_audio\n",
    "y_new = new_real_labels + new_fake_labels\n",
    "\n",
    "# Convert list to numpy array and split into train and test sets\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f597bf-a048-4c84-bf2f-3cb049fb4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract Mel spectrograms\n",
    "def extract_mel_spectrogram(audio, sr=16000, n_mels=128):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Function to pad or truncate the spectrogram\n",
    "fixed_length = 400  # Adjust this based on your dataset\n",
    "\n",
    "def pad_or_truncate(mel_spec, max_len=fixed_length):\n",
    "    if mel_spec.shape[1] > max_len:\n",
    "        return mel_spec[:, :max_len]\n",
    "    elif mel_spec.shape[1] < max_len:\n",
    "        pad_width = max_len - mel_spec.shape[1]\n",
    "        return np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mel_spec\n",
    "\n",
    "# Prepare Mel spectrograms for new dataset\n",
    "X_train_mel_new = [pad_or_truncate(extract_mel_spectrogram(audio)) for audio in X_train_new]\n",
    "X_test_mel_new = [pad_or_truncate(extract_mel_spectrogram(audio)) for audio in X_test_new]\n",
    "\n",
    "# Convert to numpy arrays and reshape for the model\n",
    "X_train_mel_new = np.array(X_train_mel_new).reshape(-1, 128, fixed_length, 1)\n",
    "X_test_mel_new = np.array(X_test_mel_new).reshape(-1, 128, fixed_length, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee94d125-8859-4e48-bf25-5e2f628d6299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 599ms/step - accuracy: 0.8644 - loss: 0.4291 - val_accuracy: 0.9123 - val_loss: 0.2053\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9396 - loss: 0.1423 - val_accuracy: 0.9825 - val_loss: 0.1274\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 569ms/step - accuracy: 0.9958 - loss: 0.0199 - val_accuracy: 0.9474 - val_loss: 0.3562\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 576ms/step - accuracy: 0.9923 - loss: 0.0271 - val_accuracy: 0.9649 - val_loss: 0.2068\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9908 - loss: 0.0189 - val_accuracy: 0.9649 - val_loss: 0.1314\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 571ms/step - accuracy: 0.9930 - loss: 0.0114 - val_accuracy: 0.9825 - val_loss: 0.1258\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 575ms/step - accuracy: 0.9958 - loss: 0.0105 - val_accuracy: 0.9825 - val_loss: 0.1439\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 582ms/step - accuracy: 0.9911 - loss: 0.0081 - val_accuracy: 0.9825 - val_loss: 0.1506\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 569ms/step - accuracy: 0.9773 - loss: 0.0238 - val_accuracy: 0.9825 - val_loss: 0.1547\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 576ms/step - accuracy: 0.9786 - loss: 0.0220 - val_accuracy: 0.9825 - val_loss: 0.1577\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 578ms/step - accuracy: 0.9859 - loss: 0.0129 - val_accuracy: 0.9825 - val_loss: 0.1559\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 561ms/step - accuracy: 0.9852 - loss: 0.0131 - val_accuracy: 0.9825 - val_loss: 0.1643\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 564ms/step - accuracy: 0.9936 - loss: 0.0123 - val_accuracy: 0.9825 - val_loss: 0.1605\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 566ms/step - accuracy: 0.9897 - loss: 0.0138 - val_accuracy: 0.9825 - val_loss: 0.1571\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9880 - loss: 0.0202 - val_accuracy: 0.9825 - val_loss: 0.1510\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 558ms/step - accuracy: 0.9873 - loss: 0.0129 - val_accuracy: 0.9825 - val_loss: 0.1544\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9900 - loss: 0.0101 - val_accuracy: 0.9825 - val_loss: 0.1579\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 566ms/step - accuracy: 0.9832 - loss: 0.0170 - val_accuracy: 0.9825 - val_loss: 0.1514\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 575ms/step - accuracy: 0.9886 - loss: 0.0114 - val_accuracy: 0.9825 - val_loss: 0.1537\n",
      "Epoch 20/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 569ms/step - accuracy: 0.9932 - loss: 0.0093 - val_accuracy: 0.9825 - val_loss: 0.1560\n",
      "Epoch 21/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 565ms/step - accuracy: 0.9938 - loss: 0.0141 - val_accuracy: 0.9825 - val_loss: 0.1502\n",
      "Epoch 22/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 563ms/step - accuracy: 0.9916 - loss: 0.0191 - val_accuracy: 0.9825 - val_loss: 0.1528\n",
      "Epoch 23/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 560ms/step - accuracy: 0.9816 - loss: 0.0198 - val_accuracy: 0.9825 - val_loss: 0.1565\n",
      "Epoch 24/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 572ms/step - accuracy: 0.9801 - loss: 0.0148 - val_accuracy: 0.9825 - val_loss: 0.1553\n",
      "Epoch 25/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 567ms/step - accuracy: 0.9844 - loss: 0.0136 - val_accuracy: 0.9825 - val_loss: 0.1543\n",
      "Epoch 26/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9899 - loss: 0.0140 - val_accuracy: 0.9825 - val_loss: 0.1537\n",
      "Epoch 27/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 572ms/step - accuracy: 0.9832 - loss: 0.0168 - val_accuracy: 0.9825 - val_loss: 0.1518\n",
      "Epoch 28/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 566ms/step - accuracy: 0.9863 - loss: 0.0165 - val_accuracy: 0.9825 - val_loss: 0.1512\n",
      "Epoch 29/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 564ms/step - accuracy: 0.9952 - loss: 0.0100 - val_accuracy: 0.9825 - val_loss: 0.1318\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 567ms/step - accuracy: 0.9971 - loss: 0.0073 - val_accuracy: 0.9825 - val_loss: 0.1254\n",
      "Epoch 31/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 562ms/step - accuracy: 0.9853 - loss: 0.0205 - val_accuracy: 0.9825 - val_loss: 0.1275\n",
      "Epoch 32/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9896 - loss: 0.0122 - val_accuracy: 0.9825 - val_loss: 0.1334\n",
      "Epoch 33/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 560ms/step - accuracy: 0.9914 - loss: 0.0158 - val_accuracy: 0.9825 - val_loss: 0.1366\n",
      "Epoch 34/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 565ms/step - accuracy: 0.9958 - loss: 0.0060 - val_accuracy: 0.9825 - val_loss: 0.1358\n",
      "Epoch 35/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 574ms/step - accuracy: 0.9899 - loss: 0.0192 - val_accuracy: 0.9825 - val_loss: 0.1372\n",
      "Epoch 36/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 570ms/step - accuracy: 0.9949 - loss: 0.0099 - val_accuracy: 0.9825 - val_loss: 0.1393\n",
      "Epoch 37/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 563ms/step - accuracy: 0.9835 - loss: 0.0233 - val_accuracy: 0.9825 - val_loss: 0.1360\n",
      "Epoch 38/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 568ms/step - accuracy: 0.9949 - loss: 0.0110 - val_accuracy: 0.9825 - val_loss: 0.1347\n",
      "Epoch 39/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 566ms/step - accuracy: 0.9887 - loss: 0.0132 - val_accuracy: 0.9825 - val_loss: 0.1381\n",
      "Epoch 40/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 563ms/step - accuracy: 0.9932 - loss: 0.0125 - val_accuracy: 0.9825 - val_loss: 0.1392\n",
      "Epoch 41/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 564ms/step - accuracy: 0.9954 - loss: 0.0084 - val_accuracy: 0.9825 - val_loss: 0.1364\n",
      "Epoch 42/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 572ms/step - accuracy: 0.9938 - loss: 0.0088 - val_accuracy: 0.9649 - val_loss: 0.1146\n",
      "Epoch 43/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 564ms/step - accuracy: 0.9880 - loss: 0.0200 - val_accuracy: 0.9649 - val_loss: 0.1169\n",
      "Epoch 44/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 566ms/step - accuracy: 0.9965 - loss: 0.0098 - val_accuracy: 0.9649 - val_loss: 0.1170\n",
      "Epoch 45/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 572ms/step - accuracy: 0.9853 - loss: 0.0226 - val_accuracy: 0.9649 - val_loss: 0.1179\n",
      "Epoch 46/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 565ms/step - accuracy: 0.9952 - loss: 0.0099 - val_accuracy: 0.9649 - val_loss: 0.1171\n",
      "Epoch 47/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 567ms/step - accuracy: 0.9864 - loss: 0.0168 - val_accuracy: 0.9649 - val_loss: 0.1157\n",
      "Epoch 48/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 569ms/step - accuracy: 0.9886 - loss: 0.0196 - val_accuracy: 0.9649 - val_loss: 0.1150\n",
      "Epoch 49/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 563ms/step - accuracy: 0.9945 - loss: 0.0116 - val_accuracy: 0.9649 - val_loss: 0.1135\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 564ms/step - accuracy: 0.9959 - loss: 0.0114 - val_accuracy: 0.9649 - val_loss: 0.1125\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = load_model('newfine_tuned_audio_deepfake_model.keras')\n",
    "\n",
    "# Fine-tune the model on the new dataset\n",
    "history = model.fit(X_train_mel_new, np.array(y_train_new), epochs=50, validation_data=(X_test_mel_new, np.array(y_test_new)))\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save('fordemo_tuned_audio_deepfake_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57d858cc-05f7-476a-8018-de2325f4c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Fake\n"
     ]
    }
   ],
   "source": [
    "def predict_audio(audio_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    mel_spec = extract_mel_spectrogram(audio)\n",
    "    mel_spec = pad_or_truncate(mel_spec)  # Pad/truncate the Mel spectrogram\n",
    "    mel_spec = mel_spec.reshape(1, 128, fixed_length, 1)  # Reshape for the model\n",
    "    prediction = model.predict(mel_spec)\n",
    "    return 'Fake' if prediction > 0.5 else 'Real'\n",
    "\n",
    "# Example: Predict on a new audio file\n",
    "audio_file = r'niruisfake.mp3'\n",
    "print(predict_audio(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbaa41ae-f6bc-4d8a-9519-51d01769e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Real\n"
     ]
    }
   ],
   "source": [
    "def predict_audio(audio_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    mel_spec = extract_mel_spectrogram(audio)\n",
    "    mel_spec = pad_or_truncate(mel_spec)  # Pad/truncate the Mel spectrogram\n",
    "    mel_spec = mel_spec.reshape(1, 128, fixed_length, 1)  # Reshape for the model\n",
    "    prediction = model.predict(mel_spec)\n",
    "    return 'Fake' if prediction > 0.5 else 'Real'\n",
    "\n",
    "# Example: Predict on a new audio file\n",
    "audio_file = r'ibte (4).wav'\n",
    "print(predict_audio(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107aa7c-d0c7-437d-97b0-a790eb8ab46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74ade20c-0b41-40bb-8fd5-45724c33a763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 16384), dtype=float32). Expected shape (None, 128, 400, 1), but input has incompatible shape (1, 16384)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 16384), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Example: Predict on a new audio file\u001b[39;00m\n\u001b[0;32m     55\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malifisFake (5).wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_audio(audio_file))\n",
      "Cell \u001b[1;32mIn[27], line 49\u001b[0m, in \u001b[0;36mpredict_audio\u001b[1;34m(audio_file, fixed_length)\u001b[0m\n\u001b[0;32m     46\u001b[0m mel_spec \u001b[38;5;241m=\u001b[39m mel_spec\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Make prediction using the loaded model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(mel_spec)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Return 'Fake' if the prediction probability is greater than 0.5, else 'Real'\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFake\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReal\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:244\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    242\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 16384), dtype=float32). Expected shape (None, 128, 400, 1), but input has incompatible shape (1, 16384)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 16384), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_test contains the actual labels and y_pred_prob contains predicted probabilities\n",
    "\n",
    "# Calculate predicted probabilities\n",
    "y_pred_prob = model.predict(X_test_mel)\n",
    "\n",
    "# Convert predicted probabilities to binary predictions\n",
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "\n",
    "# 1. ROC Curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 2. Accuracy, Precision, Recall, F1-Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f'Confusion Matrix:/n{conf_matrix}')\n",
    "\n",
    "# 4. Equal Error Rate (EER)\n",
    "fnr = 1 - tpr  # False negative rate\n",
    "eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "eer_fpr = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "eer = (eer_fpr + fnr[np.nanargmin(np.abs(fnr - fpr))]) / 2\n",
    "\n",
    "print(f'Equal Error Rate (EER): {eer:.2f} at threshold {eer_threshold:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "373c4e6f-589b-42c8-965f-e6fa1fc81dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Real\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Step 1: Load the fine-tuned model\n",
    "model_path = 'fordemo_tuned_audio_deepfake_model.keras'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Function to extract the Mel-spectrogram\n",
    "def extract_mel_spectrogram(audio, sr=16000, n_mels=128, n_fft=2048, hop_length=512):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Function to pad or truncate the Mel-spectrogram to a fixed length\n",
    "def pad_or_truncate(mel_spec, fixed_length=400):\n",
    "    if mel_spec.shape[1] < fixed_length:\n",
    "        pad_width = fixed_length - mel_spec.shape[1]\n",
    "        mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    elif mel_spec.shape[1] > fixed_length:\n",
    "        mel_spec = mel_spec[:, :fixed_length]\n",
    "    return mel_spec\n",
    "\n",
    "# Main prediction function\n",
    "def predict_audio(audio_file):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Extract the Mel-spectrogram\n",
    "    mel_spec = extract_mel_spectrogram(audio, sr)\n",
    "    \n",
    "    # Pad or truncate the Mel-spectrogram\n",
    "    mel_spec = pad_or_truncate(mel_spec)\n",
    "    \n",
    "    # Reshape the Mel-spectrogram for the model\n",
    "    mel_spec = mel_spec.reshape(1, 128, 400, 1)  # Adjust to (1, 128, 400, 1) format\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(mel_spec)\n",
    "    \n",
    "    # Return 'Fake' or 'Real' based on the prediction threshold (0.5)\n",
    "    return 'Fake' if prediction > 0.5 else 'Real'\n",
    "\n",
    "# Example: Predict on a new audio file\n",
    "audio_file = r'ibte (10).wav'\n",
    "print(predict_audio(audio_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebc9ca-cd8a-4e46-a1ee-935c32cf8ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
